## 제1장 인공지능 : 위대한 인공지능, 깨어나다

#### 250년 전 인간을 이긴 최초의 체스 기계
1770년 오스트리아에서 스스로 체스를 두는 기계, `메케니컬 터크(Mechanical Turk)` 등장

당시 엄청난 인기를 끌었는데 사람이 기계에 들어가서 인간의 지능을 활용해 체스를 두는 기계라는 사실이 80여년 뒤 밝혀짐 (*나폴레옹도 속음,,,*)

`모라백의 역설` (Moravec's Paradox): 어려운 것은 쉽고, 쉬운 것은 어렵다. 아이한테 쉬운 일이 기계한테는 어렵고, 기계한테 쉬운 일이 인간에게는 어려운 상황 (e.g. 체스말 옮기기)

#### 진정한 인공지능이 등장하다
1956년 다트머스대에서 처음으로 
`인공지능(Artificial Intelligence)`이라는 용어를 고안한 후, 세계로부터 인공지능은 엄청난 주목을 받음

- 다트머스대 '지능을 가진 기계' 학술회의에 모인 학자들이 처음으로 인공지능이라는 용어 고안하고 사용함

그 후 인간의 두뇌 구조를 본뜬 `인공 신경망(Artificial Neural Network)` 모델 등장

 인공 신경망의 초기 모델인 `퍼셉트론(Perceptron)`은 1958년 코넬 항공 연구소에서 근무하던 프랭크 로젠블랫이 개발

`퍼셉트론`(Perceptron) : 1958년에 등장한 인공 신경망의 최초 모델

당시 퍼셉트론으로 풀 수 없는 문제가 많았고 복잡한 신경망을 제대로 학습할 수 있는 방법도 알지 못해서 세상에서 잊혀져 감

이후 초창기 인공지능에서는 규칙 기반 모델이 대세가 됨


`프랭크 로젠블랫` (Frank Rosenblatt) : 코넬 항공 연구소에서 근무하던 로젠블랫은 해군의 지원을 받아 퍼셉트론 개발


#### 규칙 기반, 인공지능을 구현하다

세계 최초의 프로그래머로 알려진 에이다 러브레이스는 컴퓨터가 등장하기 전부터 현대까지도 활용되는 프로그래밍의 기본 개념을 정립

`에이다 러브레이스`(Ada Lovelace) : 세계 최초의 프로그래머 (*and she's a woman!*)

러브레이스 says "기계는 인간이 시키는 일만 한다. 어떤 해석관계나 진실을 예측할 능력은 없다" 기계는 시키는 일만 한다는 생각은 전통적인 방식의 컴퓨터 프로그래밍 개념으로 이어짐

`프로그래밍(Programmimg)`이란 규칙(알고리즘)과 데이터(자료구조)를 입력해 정답을 출력하는 과정

초기에는 인공지능에게 수많은 if-then 규칙(알고리즘)을 사람이 일일이 입력함

`if-then 규칙`이란 '만약 ~라면 ~이다'라는 조건문을 거쳐 결론을 이끌어내는 규칙으로 초기 인공지능 기술에 가장 많이 활용된 방법 
(e.g. if 신호등이 녹색이면 then 길을 건너라)

초기에는 규칙을 입력하는 방식이 잘 통하는 것 같았지만 한계 존재
규칙에서 벗어나는 경우에 추론을 하지 못함
초기에 인공지능이 암흑기에 빠져든 데는 if-then 규칙의 한계도 한 몫함


#### 머신러닝, 스스로 규칙을 찾아내다
`머신러닝 (machine learning)`: 기계가 스스로 학습하는 방식 즉, 사람이 규칙을 입력하는게 아니라 컴퓨터가 데이터에서 스스로 규칙을 찾아냄

인공지능 분야에 머신러닝을 활용하게 되면서 인공지능 분야 다시 성과를 냄
데이터를 이용해 변형에 따른 변칙까지 찾아낼 수 있게 되면서 규칙에서 벗어난 결과도 추론하게 됨

`요슈아 벤지오(Yoshua Bengio)` : 몬트리올대 교수이자 딥러닝의 대가

`페이페이 리(Fei Fei Li)` : 스탠포드대 교수

스탠포드의 페이페이리 교수는 사람이 분류한 이미지와 기계가 자동 분류한 이미지가 얼마나 일치하는지 겨루는 대회인 이미지넷 대규모 시각 인식 챌린지 개최(2010~2017) 

사람이 분류하는 일은 '아마존 메케니컬 터크'라는 플랫폼 서비스를 통해 진행. 사람들이 직접 분류한 수천만 장의 이미지 데이터를 구축할 수 있었음

매년 정확도가 1-2% 오르다가 2012년 제프리 힌튼 교수팀이 딥러닝을 활용해서 월등한 성과로 우승 차지. 힌튼 교수 팀은 컨볼루션(Convolution) 기법을 사용한 딥러닝을 활용

잊혀져가던 인공 신경망이 이미지넷과 컨볼루션을 만나 딥러닝이라는 새로운 이름으로 성과를 내기 시작. 이미지 인식은 딥러닝 기술이 가장 좋은 성과를 내고 있는 분야 중 하나(주변 사물을 인식해야 되는 자율주행에서도 사용)

`제프리 힌튼(Geoffrey Hinton)` : 토론토대 교수 
얀 르쿤(Yann LeCun)


#### 인공지능의 핵심기술, 딥러닝의 등장

딥러닝이 잘 작동되게 된 이유는 알고리즘 발전에 있음. 일부 연구자이 끝까지 연구를 이어온 덕분에 제대로 학습하는 방법을 찾아내었고 이를 딥러닝이라고 부름

`딥러닝(Deep Learning)`은 머신러닝의 일종. 머신러닝처럼 딥러닝도 데이터와 정답을 입력하면 스스로 규칙을 찾아내지만 기존 머신러닝보다 훨씬 많은 데이터를 학습할 수 있고 풍부한 규칙을 찾아낼 수 있음

인간 두뇌의 작동구조를 본떠 만든 인공 신경망에 입력 다량의 데이터를 넣고 학습을 거쳐 자동으로 원하는 결과와 최대한 비슷하게 나오도록 조절. 

따라서 데이터가 많을수록 훨씬 정교하고 조절할 다이얼이 많을수록 더 풍부하게 표현할 수 있는 모델이 완성됨. 단, 다이얼이 많다는 점은 장점이자 단점. 다이얼이 너무 많으면 도대체 어떤 것 때문에 결과값이 나오는지 제대로 파악하기 어렵기 때문.

인공지능 연구 초기에는 시스템이 왜 그런 결정을 내렸는지 이유를 설명할 수 있어야 한다는 `해석 가능성(interpretability)`를 매우 중요하게 생각함. 이 때문에 논리적인 절차를 분석해 결정한 이유를 알아낼 수 있는 if-then 규칙 기반 시스템이 대세를 이룸.

하지만, 지금의 인공지능은 인간이 해석할 수 있는 규칙을 거쳐 결론에 도달하는 것이 아니기 때문에 인공지능의 추론 과정을 분석하는 것이 의미 없는 일이 됨.

지금의 인공지능은 알고리즘만으로 구현된게 결코 아님
**알고리즘**, **데이터**, **시스템**의 삼박자가 어우러져 발전함


#### 데이터, 인공지능의 원유

2001년 MS 연구자들이 발표한 논문의 계기로 인공지능 분야에서 데이터가 중요하다는 믿음이 널리 퍼짐

충분한 데이터만 있으면 어떠한 알고리즘을 거치든 관계없이 정확도가 높아짐 (= 대중의 지혜) 결국 알고리즘보다는 데이터의 차이가 정확도를 높이는데 효과적

`대중의 지혜(Wisdom of Crowds)` : aka 집단지성(Collective Intelligence). 다양한 집단의 데이터가 많이 모이면 소수의 전문가의 의견보다 더 정답에 가까운 결과를 얻어낼 수 있다는 원리.
평범한 다수는 탁월한 소수보다 훨씬 더 현명할 수 있다는 증명하며 데이터의 힘을 나타냄

복잡한 문제일수록 거대한 데이터의 힘을 활용해 문제를 해결하는 게 훨씬 더 합리적임

"많은 데이터를 가진 간단한 모델이 적은 데이터를 가진 정교한 모델보다 더 뛰어나다" 구글 인공지능 연구 디렉터 피터 노빅, <믿을 수 없는 데이터의 효과(2009)>

오늘날 인공지능이 주목 받고 있는 데는 빅데이터의 역할이 매우 큼
인공지능의 성공은 데이터 증가의 성공


#### 시스템, GPU가 인공지능을 완성하다

과거 신경망 구조는 지금의 신경망 구조와 크게 다르지 않았고 성능도 뛰어났지만 과거와 현재의 시스템만큼은 큰 차이가 있음. 학습과정을 지금의 시스템으로 바뀌면서 딥러닝이 급속도로 발전하는 데 큰 역할을 함

`무어의 법칙(Moore's Law)`: 반도체 집적회로의 성능은 2년마다 2배 증가한다

1999년 엔비디아는 지포스 그래픽 카드, 세계 최초의 GPU(Graphic Processing Unit)를 출시하며 그래픽 카드 시장을 독주하기 시작

그래픽 카드의 작동방법 : 3차원 그래픽을 만드는 과정은 컬러링북에 색칠하는 과정과 비슷함. 게임 그래픽 카드는 붓을 한두개를 사용하는게 아니라 수천개의 붓을 한꺼번에 사용하는 효과를 냄. 코어 각각의 성능은 좀 떨어지지만 갯수 차이로 작업을 더 빠르게 처리할 수 있음(=병렬연산)

*그래픽 카드와 인공지능의 관계?*

2004년 엔비디아는 스탠퍼드에서 GPU 병렬연산을 연구하던 이언 벅을 채용
게임 뿐만 아니라 GPU를 활용할 수 있는 다양한 방법을 모색
이를 위해 CUDA(Compute Unified Device Architecture)라는 플랫폼 발표하며 GPU를 이용한 병렬연산의 시대 개막

 - 다른 회사도 GPU를 만들 수 있지만 엔비디아가 시장을 독점할 수 있었던 이유는 CUDA의 존재 때문
 - 모든 딥러닝 라이브러리가 CUDA를 우선으로 지원하고 있고 CUDA 플랫폼의 지원 또한 워낙 강력하기 때문에 CUDA를 지원하지 않는 다른 회사에서 출시한 GPU는 사용하기 어려움
 - 초기에 GPU는 수학, 물리, 화학과 같은 기초과학 분야의 수치 해석에 사용하는 과학 계산 도구였음
 - 그러다 인공지능 연구자가 인공 신경망이 대규모 병렬연산에 적합한 구조임을 발견(인공 신경망은 수많은 노드 간의 단순 계산이 반복되는 구조여서 단순한 계산을 한꺼번에 많이 처리하는 GPU의 특성과 잘 맞음)
    - 2009년 스탠퍼드대에서 인공 신경망에 GPU를 도입하여 70배나 빠르게 학습할 수 있었다는 논문을 발표했고 이후 모든 연구자들이 인공 신경망에 GPU를 도입

이후 GPU는 딥러닝의 핵심적인 하드웨어로 폭발적인 인기를 끌게 됨


#### 오픈소스, 모두가 참여하는 혁신

`오픈소스`: 프로그램의 소스코드를 공개하는 것

오픈소스를 선택하는 이유는 공개를 통해 영향력을 더욱 공고히 하기 위함
- 공개를 통해 강요에 의한 권위가 아니라 자연스럽게 영향력이 형성되며 많은 사람들이 보면서 문화 전체가 점점 더 선순환하게 되는 것과 비슷

오픈소스 문화를 선도하는 것은 **단연 IT 업계**

- 코넬대학교에서 운영하는 아카이브(arXiv)라는 논문 저장 사이트에 수많은 연구자가 모여들어 무료로 논문을 등재하기 시작. 누구나 무료로 자유롭게 아카이브에서 최신 연구를 마음껏 읽을 수 있음

이처럼 열린 연구 문화는 딥러닝 기술이 발전하는 데 엄청난 영향을 끼침

- 여러가지 딥러닝 프로그램도 오픈소스로 등장했으며, 다양한 과학 계산 라이브러리가 오픈소스로 등장하면서 연구자들은 복잡한 계산보다는 알고리즘에만 집중할 수 있는 환경 조성

- 2015년 가을 구글이 내부에서 사용하던 딥러닝 라이브러리를 오픈소스로 텐서플로(TensorFlow)라는 이름 하에 공개
    - 복잡한 신경망을 쉽게 구현할 수 있고 확장성도 뛰어남

- 페이스북은 파이토치(PyTorch)라는 딥러닝 프로그램을 오픈소스로 공개
    - 매우 직관적인 방식으로 복잡한 모델도 이해하기 쉬워서 연구자들이 논문을 쓸 때 가장 많이 활용하는 프로그램

 상용 SW를 능가하는 오픈소스의 등장은 SW의 패러다임을 뒤집었으며, 기술의 발전과 사용자 수 증가를 가속함
 SW 발전의 속도는 무어의 법칙을 넘어섬


> #### ***food for thought***
> - 오픈소스 문화가 대변하듯 IT 분야가 타 연구 분야보다 개방적인데 그 이유가 뭘까?