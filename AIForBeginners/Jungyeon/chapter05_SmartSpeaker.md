## 제5장 스마트 스피커 : 시리는 쓸모 있는 비서가 될 수 있을까

#### 인공지능 비서의 탄생
- 국내외 음성인식 서비스 및 스마트 스피커 출시
    - 해외 : Apple Siri(2011), Amazon Echo(2014), MS Cortana(2014), Google Google Assistant(2016)
    - 국내 : SKT NUGU(2016), 네이버 클로바(2017), 카카오 카카오미니(2017)
        - 카카오, 다음과 합병 후 다음의 검색엔진과 자연어처리엔진을 활용하여 6개월만에 제품 출시

- 스마스 스피커의 핵심 컨셉과 기술은 수십년 동안 연구되다가 기술이 무르익고 음성인식비서 등 하나의 제품으로 탄생됨 
    - 핵심 기술 : 에이전트 기반의 아키텍쳐, 자연어 이해, 온톨로지 등


#### 애플 시리, 음성인식 비서의 시대를 열다
- 시리는 스탠포드대 내 민간 연구소 SRI 인터내셔널의 연구 프로젝트로 시작했다가 일부 연구원이 독립해 시리라는 스타트업을 세움
- 초창기 시리는 텍스트를 입력하면 응답해주는 챗봇이었으며 음성 인식 기능 부재
- 챗봇의 형태로는 시장의 반응을 얻기 어렵다고 판단한 이사회가 출시를 1년 늦추며 음성인식 기능 도입 추진
    - 시리의 등장으로 음성인식 비서 카테고리가 탄생하게 되었으며 시리를 통해 음성 인식이라는 개념이 대중화
- 시리는 아이폰 앱으로 출시 3개월 만에 애플이 인수(2010), 애플 아이폰 4S에 정식 탑재(2011)
- 시리 초장기 멤버 애플 퇴사 후 비브랩스 설립(2012), 삼성전자 인수(2016), 빅스비 탄생(2017)


#### 아마존 알렉사, 스마트 스피커의 시대를 열다
- 첫 음성인식 비서는 시리, 첫 스마트 스피커는 아마존 에코(2014)
    - 에코의 비서 이름이 알렉사, 고대 이집트 알렉산드리아에 오마주한 이름


#### 스마트 스피커는 어떻게 말을 알아들을까?
    스마트 스피커 작동 원리
    0) 사람이 호출하면 스피커 웨이크업
    1) 사람이 질문하면 음성을 텍스트 문장으로 변환
     - 스피커에 내장된 작은 음성인식 엔진 활용
    2) 문장을 이해한 다음에는 명령 생성
    3) 명령으로 스킬을 실행한 다음에는 다시 문장 제작
    4) 마지막으로 음성을 합성하여 문장을 소리내어 읽음
- `웨이크업(wake-up)`: 지정된 호출어를 부르면 스피커가 반응하면서 깨어나는 과정


#### 사람의 목소리를 알아듣는 음성인식 과정
과거 음성인식 과정
- 음소(소리의 최소 단위)가 결합하여 단어와 구문을 만드는 규칙을 언어학자들이 분석해 if-then 규칙으로 프로그래밍
    - 음성의 파형에서 음소 인식한 후 음소의 고유한 배열을 기반으로 단어 인식
- 한계점 : 음성의 유연함을 규칙으로 정리하기 어려움
    1. 사람은 발음하는 방식이 일정하지 않으며(사람마다 음소를 다르게 발음하거나 일부 음소 생략 등) 어조에 따른 의미 변화, 동음이의어, 구문 등 구별하기 어려움
    2. 음소 중심 규칙은 음성 외 여러 요소(대화의 화자, 상황 등 맥락)을 종합적으로 고려하기 어려움


#### 음향 모델, 음성의 파형에서 단어를 인식하다
- DARPA 1971년부터 5년간 음성인식 기술을 겨루는 개발 대회를 주최
- 음성인식 기술은 오랫동안 진전이 없다가 70년대 중반, 규칙 기반보다 통계 기반이 더 좋은 성능을 낼 것으로 생각한 연구자들이 은닉 마르코프 모델 사용 
    - `은닉 마르코프 모델(Hidden Markov Model)`: 은닉된 상태와 관찰 가능한 결과로 구성된 통계적 모델
    - 관찰 결과에 따라 디코딩 과정을 거쳐 은닉된 상태를 확률적으로 예측할 수 있는 모델 (e.g. 동거인의 행동을 보고 날씨를 예측함)
    - 규칙 기반 모델보다 통계 기반 모델이 더 좋은 성능을 보여줬으며 오류율도 개선(오류율 개선 1990년 중반 40% -> 2010년 15%)되었으나 여전히 한계 존재
- 연구자들이 딥러닝에 기반한 접근법을 연구
    - 음성의 파형은 시간의 흐름에 따라 순서대로 구성되는 데이터
    - 하지만, 당시 인공 신경망은 시계열 구조를 학습할 수 없었는데 하며 시계열 형식을 학습할 수 있는 인공 신경망 구조 개발
    - `순환 신경망(Recurrent Neural Network, RNN)`: 시간의 흐름에 따라 순서대로 구성되는 시계열 형식을 학습할 수 있는 인공 신경망
    - 인터넷 시대를 맞이하며 음성 데이터가 폭발적으로 늘어났으며 녹음된 음성 데이터를 변조하며 학습 데이터의 양을 늘림
    - 딥러닝 + 빅데이터를 이용해 음성의 파형으로 단어를 인식하는 성능이 뛰어난 `음향 모델(acoustic model)` 탄생


#### 언어 모델, 오인식 단어를 보정하다
- 인간은 인지 능력 덕분에 잘못 알아들은 단어도 상식에 기반해 보정해서 이해하지만 규칙 기반 음성인식은 오인식을 보정할 수 없었기 때문에 여태까지 좋은 성과를 낼 수 없었음
- `언어 모델(language model)`: 언어 모델은 단어 시퀀스에 확률을 할당하는 일을 하는 모델. 즉, 가장 자연스러운 단어 스퀀스를 찾아내는 모델
    - 사전 지식을 학습한 언어모델은 확률 기반으로 유추와 보정을 할 수 있음
        - 앞에 들은 단어를 바탕으로 다음에 나올 단어나 동음이의어 등을 자연스럽게 유추 가능
        - 언어 모델은 음향 모델이 목소리를 잘못 인식하더라도 이를 확률 기반으로 다시 보정(마치 인간 뇌의 인지 작용처럼)
        - 규칙 기반 음성인식은 오인식을 보정할 수 있는 기능이 없었기 때문에 한계 존재


#### 자연어 이해, 언어를 이해하다
- `자연어 이해(Natural Language Understanding, NLU)`: 기계가 문장의 의미를 파악하기 위해 처리하는 과정
    1. 도메인 분류: 텍스트 문장이 어떤 카테고리에 해당하는지 판별
    2. 인텐트 분류: 문장에서 사용자 의도 파악(조회, 재생 등)
    - 위 과정을 통해 사용자의 요청사항을 구조화하며 추가로 필요한 정보는 슬롯 필링과 멀티 턴을 통해 얻어냄
- `슬롯 필링(Slot Filling)`: 누락된 정보를 채워주는 과정
    - 예: 오늘 날씨 어때? -> 지역 정보 누락되었고 지역을 특정하지 않았으니 현재 위치 정보를 채움
    - `멀티 턴(Multi-turn)`: 여러 번의 대화. 자동으로 유추하기 어려운 정보는 사람에게 질문을 하며 대화를 이어나가면서 슬롯 필링을 함


#### 다이얼로그 매니저, 명령을 실행하다
- 다이얼로그 매니저의 역할 : 
    - 여러 도메인에 맞춰 적절한 행동을 수행하도록 명령하기
        - 다이얼로그 매니저가 '음악 재생', '날씨 조회' 같은 명령을 스킬에 내리면 스킬에 등록된 서비스가 실행되어 결과를 받아옴
        - `스킬 (Skill)`: 여러 외부 서비스를 이용하는 기능
    - 대화 내용 기억하기
    - 추가 내용 외부 지식 기반 서비스 검색


#### 자연어 생성, 대화를 디자인하다
- 생성 영역에서는 딥러닝의 활용이 조심스러움
    - 100번의 발화 중 99번을 알아들으면 똑똑한 스피커지만, 100번의 문장 중 99번만 정확하게 생성했고 그 1번이 위험한 발언이면 똑똑한 스피커라고 인지하기 어려움
- 스마트 스피커는  `문제해결용 대화시스템`이라는 점에서 챗GPT와 다름
    - `문제해결용 대화시스템(Task-oriented Dialogue System)` : 날씨 조회, 식당 예약 등 목적이 분명한 대화만 하는 시스템
    - 따라서, 자유롭게 대화를 생성하지 않고 정해진 템플릿에 정보를 채워 문장 생성 (예: 현재 날씨는 __고, 온도는 __도 입니다)
    - 더욱 풍부한 대화를 위해서는 템플릿을 다양하게 구성하며 이런 작업을 두고 대화를 디자인한다고 함
    - 답이 단조롭지만 불필요한 응답을 하는 경우 거의 없음(위험한 발언, 실언 하지 않음)


#### 연결 합성, 문장을 자연스럽게 읽을 수 있을까?
- `연결 합성(Unit Selection Synthesis, USS)` : 따로 녹음한 소리를 조합해서 하나의 자연스러운 문장을 만드는 것
    - 미리 녹음된 음성을 기준에 따라 잘게 쪼개어 음편(unit)을 만들고 가장 적합한 음편을 선택(selection)하여 음성을 합성(synthesis)하는 방식


#### 음성 합성, 인간보다 더 자연스러움을 향해
- 음성을 합성하는 일은 결코 쉽지 않음
    - 구글에서 대화 디자인을 담당하는 언어학자 Margaret Urban은 "흐름, 음조, 표현, 감정, 소리 크기, 속도 등 이 모든 것이 대화의 의미를 상대방에게 전달하는 요소"라고 말함
    - 과거엔 운율, 음향, 음 길이 등 여러 매개변수의 값을 예측하고 조합해서 음성을 생성했지만 최근 딥러닝은 엔드투엔드 방식을 채택하여 보다 자연스러운 음성 생성
    - `엔드투엔드 (End-to-End)` : 입력과 출력이 한번에 진행

- 음성 합성 과정 : 1) 텍스트 > 멜 스펙트로그램 추출 2) 멜 스펙트로그램 > 음성 생성
    - 하지만, 최근에는 멜 스펙트로그램 추출 없이 텍스트에서 음성을 바로 생성하는 엔드투엔드 방식의 모델도 연구되고 있음 
    - `멜 스펙트로그램(Mel Spectrogram)`: 소리나 파동을 시각화하여 파악할 수 있도록 표현한 것. 음파와 비슷하지만 색상의 차이와 농도의 차이로 음파보다 더욱 풍부하게 표현됨
    - `보코더(Vocoder)`: 멜 스펙트로그램을 실제 음성으로 바꾸는 작업을 하는 기술


> #### ***food for thought***
> - 개인적으로 스마트 스피커를 잘 활용하지 않고 있는데 가장 큰 이유는 사용할 때마다 제대로 원하는 걸 수행해준 적이 없어서(오인식이 많아서) 오히려 스피커를 쓰는게 더 번거롭게 느껴짐
> - 결국 인간에게 유용하려면 서비스 수준이 아이언맨의 자비스 정도는 되어야 할 것 같은데 그 시점에는 스피커라는 형태도 필요 없지 않을까?라는 생각
>    - 사용되는 기반 기술은 비슷할지 언정 '스마트 스피커'라는 형태는 없어지지 않을까?
>   - 그렇다면, 기술을 담는 HW는 어떤게 될까? 결국 wearable HW가 될거 같은데 glasses, watch 등 여러 wearable 중 어떤 HW가 주류가 될지 궁금함