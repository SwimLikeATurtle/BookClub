## 제6장 기계번역 : 외국어를 몰라도 파파고만 있다면

#### 인간의 언어가 정말 어려운 이유
- 초기에는 인간의 언어와 관련된 규칙을 다 분석해서 if-then 규칙을 일일이 입력했지만 번역 품질이 좋지 않았음
- 인간의 언어가 어려운 이유:
1. 규칙이 많음 : 인간의 언어는 계속 확장되며 어떤 특정한 규칙을 따라 과학적인 방식으로 발전하지 않음
2. 많은 오류 : 모든 사람이 문법에 맞게 말하지 않으나 인간의 뇌가 보정을 할 수 있기 때문에 대화가 가능함
3. 많은 의미 : 동음이의어 같이 단어만 봐서는 정확한 의미가 파악이 안된다던지 had처럼 한 단어가 수십 가지 뜻을 가지고 있음


#### 기계번역의 시작
- `기계번역(Machine Translation)`: 인간이 사용하는 언어를 기계를 사용해 다른 언어로 번역해내는 일(1949년부터 논문에 등장)
- `시스트란(SYSTRAN)`: 1968년에 헝가리 출신 컴퓨터 과학자 피터 토마가 설립한 기계번역을 대표하는 회사


#### 규칙 기반, 모든 규칙을 정의하다
- `규칙 기반 기계번역(Rule-based Machine Translation)`: 언어학자들이 일일이 정의한 규칙을 입력하여 기계가 번역하도록 함
- 규칙을 아무리 세워도 언어의 무궁무진한 변화를 따라가기 어렵고 언어는 문화를 반영하므로 다른 언어와 의미가 딱 맞물리지 않는 경우 존재


#### 예시 기반과 통계 기반, 가능성을 보이다
- `예시 기반 기계번역(Example-based MAchine Translation)`: 기본적인 문장의 의미를 파악한 후 다음 비슷한 문장의 의미를 비교해 전체 의미를 유추하는 방식
    - 교토대 나가오 마코토 교수가 제안한 방식으로 기존의 규칙 기반 대신 풍부한 데이터를 활용하여 번역하는 방식
- 예시와 데이터가 많을 수록 정교해지지만 동음이의어를 직접처리 하는 등 많은 한계가 존재
- `통계 기반 기계번역(Statistical Machine Translation)`: 1990년대에 등장한 통계적인 방법을 접목한 기계번역 방식. 문장을 단어 또는 구문 단위로 분할한 다음 각각의 단어 의미를 번역 확률 순으로 나열해 이를 번역하고 다시 문장으로 합치는 과정
- 통계 기반 기계번역으로 번역을 해도 어색하다면 모든 경우를 조합해서 가장 자연스러운 문장을 찾음
- 더 자연스러운 결과값을 위해 단어 단위에서 구문(phrase) 단위로 확장(예: had, lunch > had lunch)
    - 단어 기반은 문장의 확률을 점검하는 규칙을 만들어야 했지만 구문 기반은 그럴 필요가 없어서 더욱 단순해짐


#### 인공 신경망 기반, 마침내 혁신이 시작되다
- 2010년대 조경현 교수 구문 기반으로 분석하는 기계번역 방식에 딥러닝 적용해서 성공적인 결과를 냄
- `신경망 기반 기계번역(Neural Machine Translation)`: 구문 단위를 넘어 문장 전체에 딥러닝을 적용한 기계번역 방식으로 문장 전체를 통으로 번역해서 훨씬 자연스러운 번역이 가능
- 문장을 통으로 압축해 숫자로 표현한 벡터 생성 > 값을 번역할 언어로 옮긴 다음 풀어서 번역문 생성
    - 벡터: 방향과 크기를 나타내는 값
    - 각각의 숫자에서 가장 확률이 높은 번역문을 찾아내기 때문에 규칙 기반처럼 단어와 단어 간의 관계, 순서를 파악하거나 통계 기반처럼 단어나 구문을 확률로 번역해 조합하고 자연스럽게 만들기 위해 애쓰지 않아도 됨
    - 엄청나게 많은 데이터를 학습하며 점점 더 번역 성능 제고(예: 문법을 모르지만 영어를 할 수 있는 유학생처럼 번역함)


#### 어텐션, 가장 혁신적인 발명
- `인코더(Encoder)`: 문장 전체의 의미를 압축하는 작업
- `디코더(Decoder)`: 압축된 벡터를 받아서 순서대로 문장을 푸는 작업
- 문장 번역이 끝날 때까지 디코더는 계속해서 인코더가 압축한 벡터를 참조하면서 더 자연스러운 문장을 만들어내는 방식으로 인공 신경망을 활용한 기계번역은 엄청난 성능을 보임
- 하지만 이런 방식에는 두가지 문제가 있었음
1. 번역할 원문의 길이와 관계없이 원문을 일정한 길이의 벡터로 한번만 압축함
2. 한번 만든 벡터를 계속 참조하다 보니 번역문이 길어질수록 핵심 단어를 놓침
- 이를 보완하기 위해 조경현 교수는 2014년 Attention이라는 개념 공동 제안
- `어텐션(Attention)`: 중요한 단어에 별도의 가중치를 부여해서 강조하는 원리
    - 어텐션으로 앞선 두가지 한계 모두 극복(번역문이 길어질 수록 벡터도 길어짐, 압축 시 중요한 단어 강조)
- 어텐션으로 기계번역의 성능이 좋아졌으며 특히, 긴 문장에서 높은 성능을 낼 수 있었음
- `트랜스포머(Transformer)`: 문장 속 단어와 같은 순차 데이터 내의 관계를 추적해 맥락과 의미를 학습하는 딥러닝 모델
    - 기존의 복잡한 딥러닝 구조 대신 인코더에 한 종류의 어텐션, 디코더에 두 종류의 어텐션으로 구성하는 등 오로지 어텐션만으로 모델 구성
    - 최근 신경망 기반 기계번역은 모두 트렌스포머 모델을 기반으로 함
        - 어텐션은 기계번역 성능을 보조하는 역할에서 이제는 핵심 알고리즘이 됨
        - 트랜스포머 모델은 이제 자연어 처리를 넘어 이미지나 음성인식에까지 널리 쓰임


#### 인간을 뛰어넘은 기계번역
- 구글 검색서비스를 운영하며 엄청나게 많은 데이터를 수집한 구글은 이를 이용해 고품질의 번역 서비스를 단기간 내에 개발
    - 2004년 구글 처음으로 번역 서비스 제공
    - 2006뇬 통계 기반의 기계번역 서비스 출시 
- 통계기반 도입 후로는 문장 데이터가 많을수록 정교하게 확률을 계산할 수 있어서 빅데이터 플랫폼을 갖고 있는 회사가 유리했음(구글 유엔과 유럽의회 회의록을 활용) 
- 2017년 카카오가 카톡 챗봇 형태의 카카오 i 번역 서비스 제공
- 카카오는 번역 서비스를 낼 생각이 없었는데 검색엔진 개발팀장이 사이드 프로젝트로 신경망 기반 기계번역을 만들다 카카오에서 정식으로 출시함
- 네이버 파파고 출시 당시에는 통계 기반 기계번역이었음. 나중에 신경망을 적용한 번역 서비스 출시는 구글보다 빨리 함
    - 네이버 파파고를 만든 사람들 -> 현대차 사내 번역 서비스 작업


#### 바벨탑, 인간은 신의 형벌을 극복할 수 있을까?
- 기계번역이라는 용어가 등장한지 약 70년이 지남
- 오랜 침체기에 빠진 기계번역은 인공 신경망을 접하고서 성능이 대폭 개선됨
- 기계번역은 딥러닝의 가장 성공적인 사례라고 여겨지기도 함


> #### ***comments***
> - 기술적인 측면 뿐만 아니라 언어 발전 단계에 대해 알게 되어서 재밌었음
> - 본문에 언급된 <음식의 언어>라는 책 찾아보기
> - 인공신경망의 번역 과정을 보며 공감(?)이 많이 되었음.어릴 적 미국에 살면서 영어에 대한 데이터가 자연스럽게 많이 쌓여서 어떤 표현이 맞다는 것만 직감적으로 알지 문법적으로 이유를 설명 못함. 그래서 간혹 내 번역문을 다른 사람에게 설명할 때에 내가 느끼는 어려움과 상대가 느끼는 당혹스러움을 인공지능 개발 이야기를 읽으며 떠올리게 되었다는 점이 재밌었음


> #### ***food for thought***
> - 책에서 나온 것처럼 기계번역의 완성도가 계속 높아지는데 그렇다면 인간이 직접 제2외국어를 배우는 것이 무슨 의미일까?