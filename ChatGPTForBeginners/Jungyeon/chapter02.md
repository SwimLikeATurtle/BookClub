## 제2장 기계번역을 정복한 인공지능

#### 너무 어려운 인간의 언어
- `기계번역(Machine Translation)`: 인간이 사용하는 자연어를 기계를 사용해 다른 언어로 번역해내는 일
- 번역은 자연어 처리 분야에서도 가장 어려운 작업 중 하나인데 크게 3가지 이유 때문:
	1. 너무 많은 규칙
언어는 일정한 규칙에 따라 체계적으로 발전하지 않음 불규칙적으로 진화하기 때문에 기계가 일정한 규칙에 맞춰 언어를 처리 하기란 매우 까다로운
    2. 너무 많은 오류
일상적인 대화에는 우리가 많은 인간에 대해는 스스로 이런 오류들을 보정 하고 이해할 수 있음
하지만 이걸 기계 알려 주는 것은 굉장히 어려워
    3. 너무 많은 의미
같은 발음에 단어가 여러 뜻을 갖는 경우가 있음 컨텍스트를 살펴 봐야 비로소 정확히 무엇을 의미 하는지 알 수 있음


#### 인공 신경망, 언어에 도전하다
- 인공신경망 기반은 문장 전체를 마치 하나의 단어처럼 통째로 번역해서 훨씬 더 자연스러운 번역이 가능(2016년 Google 번역 인공신경망 도입)
- 인공신경망이라는 거대 모델과 이를 견인 할 수 있는 방대한 데이터를 확보하면서 이것이 가능해짐
	- 역설적이게도 문장을 통째로 번역하면서 번역 과정 자체는 오히려 훨씬 단순해짐 다만 방대한 데이터가 필요함
- 신경망의 번역 과정
	1. 문장을 통째로 vector 로 압축 함
	2. 벡터 값을 번역할 언어로 옮김
	3. 벡터를 계산해서 가장 확률이 높은 단어를 차례대로 찾아 내면서 번역문을 만듬
- 벡터로 압축한다는 것은 텍스트 데이터를 컴퓨터가 이해하고 처리할 수 있도록 해당 텍스트의 특징을 반영한 여러 숫자로 요약하여 고정된 크기의 벡터로 표현 하는 것을 의미

- 문장을 압축하는 과정
	1. 문장을 띄어쓰기 단위로 구분한 다음 차례대로 인공신경망을 통과하며 핵심적인 특징을 추출
	2. 여러번 계산을 거쳐 최대한 압축
	3. 문장 전체의 의미를 압축한 벡터가 나옴

- `인코더(Encoder)` : 문장을 압축하는 부분
- `디코더(Decoder)` : 압축된 문장을 푸는 부분. 압축된 벡터를 받아서 순서대로 풀어냄
	- 한 단어씩 차례대로 보는데 두가지 입력을 받음 첫번째는 앞선 단어의 번역 두번째는 인코더가 문장 전체를 압축한 백터

- `순환신경망(Recurrent Neural Network, RNN)` : 언어가 앞뒤로 연관 관계를 갖는다는 데서 착안해 계속해서 이전 데이터를 참조 하면서 시간의 흐름에 따라 순서대로 구성 되는 시계열 형식을 학습 할 수 있는 구조
    - 다만 문장이 길어질 경우 앞에 내용을 잊어버리는 경우가 흔했음
    - 이를 극복하기 위해 2014년 어텐션 이라는 혁신적인 개념이 등장

- `어텐션(Attention)` : 중요한 단어에 별도로 가중치를 부여해서 문장을 압축할 때마다 표시. 특히 긴 문장에서 높은 성능을 발휘
- 어텐션은 한 단어 한 단어 번역 할 때마다 인코더에서 새로운 값을 만들어 냄 게다가 중요하다고 판단 되는 단어에는 가중치가 부여 되어 내려옴
- 매 단계마다 인코더가 가중치를 다르게 적용하여 압축 백터를 내려 보냄. 따라서 중요한 부분에는 가정치가 더 높음
- 어텐션은 처음에 기계번역의 성능을 보조하는 역할로 등장했으나 어텐션 도입으로 번역 품질이 향상 되었고 이제는 기계 번역의 필수 요소가 됨


#### 트랜스포머 모델, 언어를 이해하다
- `트랜스포머(transformer)`: 딥러닝 아키텍쳐인 트랜스포머는 기계번역의 성능을 높여놓음
- 트랜스포머의 구조 : 인코더와 디코더로 구성된 구조의 핵심만 나열했을 때 신경망 기반 기계번역의 원칙을 따름. 다만, 기존 RNN과 같은 순환구조를 사용하는 대신 오로지 어텐션만으로 모델을 구성함(따라서 순환 신경망이 아님)
- 기계번역에서 놀라운 성능을 보인 계기로 연구자들은 트랜스포머 모델 자체게 관심을 갖게 되었고 다양한 응용 모델 개발
    - 이 중 가장 유명한 모델은 구글의 BERT와 오픈AI의 GPT
    - BERT는 출시 당시 성능 벤치마크 결과에서 분류, 감정 분석, 독해 등으로 구성된 10가지 자연어 처리 작업에서 모두 최고점을 기록(일부 항목에서는 인간의 점수를 넘어서기도 함)
    - BERT 개발 당시 일부러 틀린 단어를 삽입하여 과적합을 방지하며 학습 시킴
- GPT는 트랜스포머 모델의 디코더 구조를 채택. 디코더는 압축된 정보를 풀어서 번역문을 생성하는 역할을 함. 따라서 디코더를 이용하는 GPT는 언어를 생성하는 모델.
    - 버트는 문장을 이해하기 위해 문장 일부를 가리며 학습했지만 GPT는 문장을 생성해야 되기 때문에 뒤에 나올 단어를 가리고 그것이 무엇인지 맞히면서 학습
- 언어를 생성한다는 측면에서 GPT는 사실상 진정한 인공지능에 가장 맞닿아 있는 기술


> #### ***comments***
> - p.69 구글 BERT의 등장으로 자연어 처리 연구자들이 일자리가 사라질 것을 두려워 했지만 몸값이 더욱 높아졌다는 부분이 흥미로웠음. 인간의 머리로는 미래를 예측하기 어려움
> - p.71 BERT는 구글에서 공개한 딥러닝 프레임워크인 텐서플로우에서만 작동했는데 이걸 당시 인기를 끌던 메타에서 공개한 파이토치에서 동작하도록 개조한 회사(허깅스페이스)가 GPT도 연동하면서 기업가치 6조원이 넘는 회사가 되었다는 부분을 읽고 허깅스페이스라는 회사가 궁금해짐. 비즈니스 기회를 잘 포착했구나라는 생각이 듦
> - p.75 60여 년 전 일라이자에게 마음을 뺏긴 사람들을 보며 인간은 참 외로운 존재라는 것을 다시 느낌