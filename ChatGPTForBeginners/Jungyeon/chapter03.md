## 제3장 챗GPT를 완성한 비밀 레시피

#### 세상을 바꿀 GPT 역사의 시작
- 오픈AI가 GPT를 발표했을 때 당시 그들의 주된 연구 분야가 `강화학습(Reinforcement Learning)`이었고 언어모델은 크게 신경 쓰지 않음
    - 강화학습은 기계가 스스로 학습하며 성능을 향상 시키는 방식을 말함
- `매개변수(Parameter)`: 모델의 학습 과정에서 조정되는 가중치 값들을 의미
    - 하나하나 다이얼로 조정할 수 있는 값이며 값들은 학습 과정에서 자동으로 조정됨
- 초기에는 BERT의 성능이 훨씬 앞서 갔고 각종 벤치마크에서 최고점을 찍음. 이에 반해 GPT는 그저 문장을 생성하기만 하는 용도가 애매한 모델이었음
    - 그럼에도 오픈AI는 GPT를 계속 발전 시켜나감
- 모델이 커지며 성능이 좋아졌고 GPT-3가 엄청난 크기의 모델을 제시하면서 각 기업들의 모델 크기 경쟁이 본격화됨
- 2022년 11월, 오픈AI가 챗GPT(GPT-3.5)를 발표하고 세상에 엄청난 파란을 불러일으킴


#### 임베딩과 토큰, 언어를 효율적으로 쪼개는 방법
- 챗GPT의 근간이 되는 GPT 언어 모델은 어떻게 만들까?
- 컴퓨터는 계산으로 작동하기 때문에 모든 것을 숫자로 표현해야 함. 오로지 0과 1만 이해하는 디지털 기계가 인간의 언어를 이해할 수 있도록 하기 위해 각 단어의 특징을 추철하여 숫자로 표기
- `임배딩(Embedding)` : 단어나 문장을 숫자 벡터로 표현하는 과정
- `벡터(Vector)` : 텍스트 데이터를 컴퓨터가 이해하고 처리할 수 있도록 수학적으로 표현한 숫자 배열
- 임베딩은 크면 클수록 좋은건 아님. 우선 차별화된 특징을 그렇게 많이 추출하기도 어렵고 추출한다고 해도 대부분의 값은 동일한 값이 될 가능성이 높음(즉, 특징을 제대로 표현하고 있지 않음)
    - 오히려 100차원 정도라도 값이 적절히 분배될 수 있는 최적의 임베딩이 좋음
- 오픈AI에서 제공하는 임베딩 API는 3,072차원(각각의 단어가 3,072개의 특징을 지닌 숫자를 갖고 있는 셈)
- 문장의 의미를 나타내기 위해서는 문장의 의미를 나타낼 수 있는 최소 단위로 쪼개면 됨
    - 다만, 쪼갠 단위 하나하나의 특징을 수치화 해야하는데 음절이나 글자 단위로 쪼개면 특징화하기 어렵고 그 의미를 정확히 파악했다고 하기 어려움
    - 챗GPT 초기 버전은 초성, 중성, 종성 단위로 한글을 쪼갬(eg. 안경 -> ㅇ,ㅏ,ㄴ,ㄱ,ㅕ,ㅇ) 따라서 국내 기업들이 챗GPT는 한글에 적합하지 않다며 문제 제기
        - 그럼에도 불구하고 챗GPT는 많은 데이터를 학습했기 때문에 한국어를 제법 잘함
        - 한글을 너무 쪼개놔서 비효율 문제 발생(eg. 점심이라는 단어는 1개로 충분히 해결할 수 있는데 6개의 낱자로 처리)
- 언어모델을 학습할 때, 문장을 쪼개는 과정을 자동으로 처리
    - 많은 문장을 읽으면서 각각의 단어가 등장하는 빈도수를 계산 (아침을 먹었다/점심을 먹었다/저녁을 먹었다)
    - 빈도가 높은 부분을 토큰으로 정함 (을 먹었다 = 하나의 토큰)
    - 나머지 단어를 각각 하나의 토큰으로 지정(아침, 점심, 저녁 = 3개의 토큰)
    - 자주 등장하는 토큰을 사전 형태로 구성하고 새로운 문장이 입력되면 토큰 사전에 포함된 단위로 쪼갬
        - 자주 등장하는 단어로 구성되므로 각각의 토큰 대부분은 의미가 살아있음
    - 토크나이저를 이용해 문장을 토큰 단위로 토크나이징함
    - 각 토큰의 임베딩 벡터(토큰의 특징을 수치화한 값)를 가져옴
    - 언어 모델은 입력값으로 받은 숫자로 계산을 함

- `토큰(Token)` : 언어 모델에서 학습을 진행하는 최소 단위(단어 갯수와 비례하지 않음). 수많은 문장을 읽어들인 다음 이에 대한 통계적인 결과로 구성
    - `토크나이징(Tokenizing)` : 학습할 수 있는 최소단위로 쪼개는 과정 즉, 토큰으로 만드는 과정 
    - `토크나이저(Tokenizer)` : 토크나이징하는 도구


#### 어텐션, GPT의 핵심 알고리즘
- 어텐션은 중요한 단어에 가중치를 주는 개념
- GPT는 어텐션만으로 구성된 트랜스포머 모델의 디코더 방식을 채택
- 어텐션의 계산 방식
    - 트랜스포머 구조에서 어텐션은 입력값을 먼저 Q, K, V 라는 세 개의 값으로 나눔
    1) `쿼리(Query)` : 찾고 싶은 주제
    2) `키(Key)` : 고유의 식별 값
    3) `값(Value)` : 정보

- GPT가 사용하는 어텐션은 동일한 문장에서 서로 간의 관계를 찾아야 해서 `셀프 어텐션(Self Attention)`이라고 부름
- `어탠션맵(Attention Map)` : 데이터 크기를 색의 강도로 나타내어 어느 부분에 집중해야 하는지 보여주는 히트맵 형태의 구조. 각 토큰 간에 관계의 강도를 나타내는 히트맵
- `마스킹(Masking)` : 아직 등장하지 않은 단어를 참조하지 않도록 가려주는 행위
- `마스크드 셀프 어텐션(Masked Self Attention)` : 디코더에서 아직 생성되지 않은 미래의 토큰을 보지 못하도록 현재 시점 이후의 모든 입력 토큰에 가림막(마스크)를 씌우는 어텐션 메커니즘
- `멀티 헤드 어텐션(Multi-Head Attention)` : 어텐션 연산을 한 번이 아닌 여러 개의 헤드로 나누어 병렬적으로 수행하여, 모델이 한 번에 다양한 관점과 관계 정보를 동시에 학습할 수 있게 하는 메커니즘. GPT의 근간이 되는 트랜스포머 모델의 핵심
- `마스크드 멀티 헤드 셀프 어텐션(Masked Multihead Self Attention)` : 마스크드 셀프 어텐션 기능을 여러 개의 헤드로 확장하여 디코더가 미래 정보를 차단하면서도 다양한 종류의 문맥 정보를 동시에 포착할 수 있게 하는 메커니즘

- 여러 차례 동시에 어텐션을 진행해주니 다른 어떤 모델보다 월등히 높은 성능을 낼 수 있었음. 따라서 토큰과 토큰 사이의 관계를 수많은 어텐션을 통해 찾아내는 것이 바로 트랜스포머의 모델의 핵심
    - 인간이 언어를 이해하는 과정과 비슷. 책을 읽으면 전체 책을 다 암기하기보다 전체적인 맥락과 함께 중요한 내용 등만 기억에 남는 것처럼
- 어텐션은 96번씩 동시에 계산한 후 이 과정을 여러차례 반복하는데 멀티 헤드 어텐션으로 96번씩 동시에 계산하면서 이 과정을 총 96회 반복한다는 뜻
- 왜 반복하는 것일까? 반복 횟수가 줄어들수록 더 빨리 동작하지만 모델이 크고 깊을수록 성능이 좋아지기 때문에 최고의 성능을 내기 위해 여러 차례 반복하는 것
    - 평균적으로 문장 하나에 500 토큰이 필요로 하는걸 감안하면 이를 위해 어텐션만 460만번 이상을 계산함


#### 스케일링 법칙, 크면 클수록 좋다
- `손실(Loss)` : 모델의 예측이 실제 정답과 얼마나 다른지를 나타내는 수치. 낮은 손실값은 모델의 예측이 정확하다는 것을 의미. 매개변수가 클수록 손실이 낮음
- 2001년 MS 연구자들은 알고리즘보다 데이터의 차이가 정확도를 높이는 데 훨씬 더 많은 영향을 미친다는 것을 밝힘
- 2009년 구글 인공지능 연구 디렉터 피터 노빅(Peter Norvig)은 많은 데이터를 가진 간단한 모델이 적은 데이터를 가진 정교한 모델보다 뛰어나다고 주장
- `클라이버 법칙(Kleiber's Law)` : 크기가 커질수록 에너지 효율이 좋아진다는 뜻
    - 1930년대 초 스위스의 농생물학자인 막스 클라이버가 동물의 대사율과 체중 사이의 관계를 두고 크기가 커질수록 에너지 효율이 좋아진다고 밝힘 (eg. 쥐, 고양이, 호랑이 세 동물의 몸집을 비교했을 때 배로 크기가 차이나지만 먹는 양, 또는 생존에 필요로 하는 에너지 양은 단위 체중당 대사율이 낮아지기 때문에 크기의 배수와 같지 않고 더 적게 필요로 함)
- 2020년 오픈AI는 논문을 통해 더 큰 모델이 더 효율적이라는 것을 증명. 큰 모델은 작은 모델에 비해 훨씬 적은 양의 데이터로도 비슷한 성능을 낸다는 것을 보여줌. 그 이유는 큰 모델이 학습과정에서 더 풍부한 표현력을 갖기 때문
    - `신경망 스케일링 법칙(Neural Scaling Law)` : 모델 크기와 데이터 양, 학습 비용(계산량)이 적절히 증가하면 모델 성능도 이에 비례해 개선된다는 것. 즉, 클수록 좋다는 규모의 법칙을 도출해냈고 초거대 언어 모델의 등장을 알리는 신호탄이 됨
    - 2022년 구글 딥마인드가 후속 연구를 통해 모델의 크기와 성능 간의 관계를 설명하는 경험적인 법칙을 제안  
    - 스케일링 법칙은 딥러닝 모델의 성능을 예측하고 제한된 자원으로 최적의 모델을 설계하는데 핵심적인 지표가 됨


#### RLHF, 챗GPT를 완성하는 비밀 레시피
- `사전 학습(Pre-Training)` : 기본적인 언어 모델을 학습하는 과정. 라벨링은 필요 없고 품질 좋고 제대로된 문장이 많기만 하면 됨
    - 인간의 개입이 필요 없기 때문에 데이터를 구축하기 훨씬 수월함
    - 많은 문장을 학습한 모델은 현재까지의 토큰을 기준으로 다음에 어떤 토큰이 나올지 예측
    - 문장 속에 포함된 토큰과 일치하지 않는다면 계속해서 손실값을 주면서 정답과 일치하도록 모델을 계속해서 수정해나감
        - 틀린 값이 나오면 손실값을 받게 되고 이 손실의 일정 비율만큼 확률이 재조정됨. 예측 토큰이 정답이 될 때까지 계속해서 반복 학습 진행

- 많은 문장을 집어넣고 다음에 나올 토큰이 무엇인지만 맞추면 되기 때문에 이처럼 단순한 사전 학습 방식이 지금의 LLM이 있게 한 1등 공신
    - 이런 과정을 통해 만든 모델을 `사전 학습 모델(Pre-trained Language Model)`이라고 함
- 단, 언어 모델은 다음 단어를 예측할 줄 아는 것이지 아직 대답을 잘하는 모델은 아님. 대답을 잘하는 챗봇이 되려면 추가로 다른 과정을 거쳐야 함

- 오픈AI는 답을 잘하는 모델을 만들기 위해 사람이 지시하는 대로 정확하게 따르는 모델을 만들고 싶었음
    - `지시 모델(Instruct Model)` : 사람이 요청한 프롬프트를 잘 따르도록 개선한 모델
    - 기존 모델에 사용할 프롬프트를 작성할 때는 언어 모델이 '다음 단어'를 예측한다는 점과 방대만 문장에서 단어의 통계적 분포에 따라 작동한다는 것을 고려해서 내가 작성한 프롬프트 뒤에 원하는 정답이 나오도록 프롬프트를 작성해야 그럴듯한 답이 나옴
    - 하지만, 오픈AI는 사람과 대화하듯이 뭉뚱그려 질문해도 사용자 프롬프트를 찰떡같이 잘 알아듣고 사람이 의도한대로 대답하는 모델을 개발하고 싶었음
    - 이들은 `미세조정(Fine Tuning)`이라는 기술을 적용
    - `미세조정(Fine Tuning)` : 사전 학습된 모델을 특정 작업이나 도메인에 맞춰 추가로 학습시키는 과정
    - 사용자 프롬프트를 따르는 미세 조정을 총 3단계 과정에 걸쳐 진행
        - 1단계) 데이터셋을 구축하고 `지도 미세 조정(Supervised Fine Tuning)` 모델을 학습
            - 기존 사전 학습 모델에 인간이 정제한 데이터를 넣고 더 다듬어 특정 작업이나 도메인에 맞게 모델을 조정하는 작업
            - 이를 통해 구축된 모델은 `SFT 모델`이라고 칭함. 인간이 지도한 내용으로 미세하게 조정한 모델이라는 뜻
            - 오픈AI는 좋은 모델을 만들기 위해 좋은 데이터를 구축하려고 노력함
                - 다양한 성별, 인종, 국적, 연령대로 팀을 구성해서 고품질 데이터를 구축하기 위해 노력했고 데이터 구축에 참여하지 않은 사람을 고용해 학습 데이터의 공정성을 따로 평가
            - SFT모델에 `인간 피드백을 이용한 강화학습(Reinforcement Learning from Human Feedback, RLHF)` 도입
                - 오픈AI는 2015년에 강화학습을 중심으로 설립된 회사. 범용 인공지능의 핵심은 강화학습에 있다고 생각함(구글 딥마인드와 함께 강화학습으로 유명한 두개 기업 중 하나)
        - 2단계) 비교 데이터를 구축하고 `보상 모델(Reward Model)`을 학습 
            - RM 모델은 하나의 질문에 대해 여러 SFT 모델이 내놓은 여러 답변을 두고 어떤 답변이 만족스러운지 사람이 선호도를 평가하는 방식으로 비교 데이터를 구축해 보상을 학습
            - `보상(Reward)` 함수로 `근접 정책 최적화(Proximal Policy Optimization, PPO)`라는 알고리즘 사용
            - 피드백을 받아 개선해나가는 과정은 할루시네이션을 줄이는데 도움이 됨
        - 3단계) RM 모델을 이용해 보상을 최적화하는 단계 (= 실제 강화학습을 이용해 성능을 높이는 단계)
            - 3단계 중 가장 복잡하고 핵심적. PPO라는 강화학습 알고리즘을 언어에 적용. 강화학습에서 보상을 반영하는 알고리즘이며 이런 원리로 만든 모델을 PPO 모델이라고 함
            - 1단계 SFT 모델이 출력한 문장을 2단계 PM 모델이 확인하여 사람들이 얼마나 선호하는 내용인지 평가해서 선호하면 보상을 많이 주고 선호하지 않는다면 보상을 적게 줌. 3단계에서는 이렇게 받은 보상을 PPO 알고리즘을 통해 기존 SFT 모델에 반영하여 성능을 개선하면서 PPO 모델을 구축
            - RM 모델을 기반으로 PPO 알고리즘이 전 과정을 자동으로 진행(인간 개입 없이)해서 PPO 모델이 탄생
        - 사전 학습을 거친 모델이 사용자 프롬프트를 잘 따르도록 만드는 과정을 `사후 학습(Post-Training)`이라고 함
- 오픈AI는 2022년 11월 사후 학습을 마친 모델을 챗GPT라는 이름으로 공개(GPT-3 공개한지 2년반 만에 출시)
    - 언어 모델이 사용자 프롬프트를 잘 따르게 되자 프롬프트 엔지니어링이 급부상
    - 언어 모델을 직접 건드리는 일은 고급 엔지니어가 필요할 뿐만 아니라 GPU를 포함한 장비 구매에 엄청난 비용이 듦. 대신 프롬프트 엔지니어링을 잘하면 혁신적인 결과물을 만들어낼 수 있기 때문에 스타트업이나 자본이 부족한 기업에서는 이러한 방식으로 초거대 언어 모델 비즈니스를 함
- 아이폰이 등장하면서 새로운 시장이 열렸던 것처럼 LLM의 등장으로 새로운 시장이 열림
- 과거 아이폰 앱을 잘 만드는 것이 과거 스마트폰 시대의 성공 방정식이었다면 LLM 시대의 성공 방정식은 프롬프트 엔지니어링이 중심이 될 것


> #### ***comments***
> p.86 매개변수가 큰 모델이 나오면서 여러 작업에서 좋은 성능을 내고 창발성(emergence)이 생긴다는 논문도 나옴
>


> #### ***food for thought***
> - 아이폰이 등장하면서 새로운 시장이 생긴 것처럼 LLM의 등장으로 어떤 시장이 생겨날까?