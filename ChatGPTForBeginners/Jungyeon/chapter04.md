## 제4장 초거대 모델 최적화 기술

#### 초거대 모델을 여러 GPU에 담는 법
- GPU, 초거대 모델을 운영하기 위해 필요한 고성능 장비 
	- 인공지능 모델이 학습하거나 추론할 때 필요한 계산을 동시에, 또 빠르게 수행
		- 최상급 GPU는 고가인데 인공지능 연구를 위해서는 여러 장 필요(엔비디아 H1000 장당 6천만원)
		- GPT-3 기준 한 문장을 생성하기 위해 GPU 5장 필요
	- GPU가 많이 필요한 이유는 메모리가 부족하기 때문
		- GPU는 자체 칩에 탑재된 메모리만 사용할 수 있는 구조(GPU 당 80GB)
			- CPU가 쓰는 메모리와 GPU가 쓰는 메모리는 구분되어 있음
		- 매개변수 하나가 2바이트라고 치면 인공지능 모델이 토큰 하나를 생성하기 위해 약 350GB의 메모리를 GPU 갯수마다 쪼개서 매번 읽어야함(GPU별 70GB)
- 인공지능 모델이 커질수록 GPU가 더 많이 필요한 구조적 문제
- `HBM(High Bandwidth Memory)` : GPU에 탑재된 초고속 메모리
	- SK하이닉스가 엔비디아에 독점적으로 납품
- 인공지능 모델을 여러 GPU에 나누는 방법
	1) `텐서 병렬화(Tensor Parallelism)` : 모델을 여러 개의 GPU에 분산하여 처리
	- 행렬의 값을 GPU별로 필요한 만큼만 들고 있고 입력값을 이용해 필요한 만큼만 계산한 후 그 결과를 합쳐서 최종 결과를 얻음
	2) `파이프라인 병렬화(Pipeline Parallelism)` : 모델을 쪼개는 방식이 아니라 처리되는 순서를 분할하는 방식
	- 계산 순서를 잘게 쪼개어 여러 레이어에서 순차적으로 처리
	- `레이어(Layer)` : 계산 단계를 지칭하는 용어로 특정 연산을 수행하고 그 결과를 다음 레이어로 전달하는 기본 구성 요소 (GPT-3는 레이어가 96개)
	- 단점 : GPU가 동시에 일하지 못하고 차례대로 일하기 때문에 1번 GPU가 일을 끝낼 때까지 다음 GPU는 계속 대기해서 비효율적


#### 매개변수는 몇 바이트일까요?
- `비트(Bit)` : 0과 1로 구성된 디지털 세상의 가장 작은 단위
- `바이트(Bite)` : 0 또는 1이 8개 있는 것 (2바이트 = 16개)

- 매개변수 하나를 2바이트로 가정하면 16칸의 메모리 영역이 필요한 것
	- 175B 모델은 1750억개의 매개변수를 보유 따라서 350GB 메모리 필요
- 메모리가 80GB인 엔비디아 H100 GPU로 175B 모델을 구동하려면 GPU 5장 필요

- 매개변수는 주로 -1에서 1 사이의 실수
	- 실수(Real Number) : 유리수와 무리수를 포함한 수직선 위에 나타낼 수 있는 모든 수
- 다만, 컴퓨터는 무한대의 실수를 표현할 수 없기 때문에 근삿값으로 표현(실수를 2진수로 표현하는 것은 복잡하지만 정수를 2진수로 표현하는건 쉬움)
- `부동소수점(Floating Point)` : 실수를 잘 표현할 수 있는 방식
	- 부동소수점은 과학적 표기법과 마찬가지로 지수부와 가수부가 있음
	- 예) 과학적 표기법으로 4.57289e-16 = 0.000000000000000457289
		- 4.57289 = 가수(Mantissa) = 유효숫자
		- e-16 = 지수(Exponent) = 10^-16을 의미
	- 부동소수점의 기본값은 32비트(float32라고도 부름) = 4바이트. 지수부는 표현범위, 가수부는 유효숫자를 나타냄, 다만 2진법을 사용하며 음수 여부를 구분하는 부호 비트가 있음
		- 예) 32비트 중 '부호 1비트 - 지수부 8비트 - 가수부 23비트'로 구성
- 매개변수 하나당 4바이트로 표현하면 너무 많은 메모리가 필요하기 때문에 절반으로 줄인 float16을 사용
- `반정밀도(Half-Precision)` = `Float16` : 비트를 적게 사용하면 수를 정교하게 표현하기 어렵기 때문에 정밀도를 반으로 낮췄다고 해서 반정밀도라고 부르며 16비트 사용
	- 다만, 지수부가 고작 5비트 밖에 안되서 표현범위가 매우 좁음
	- 딥러닝에서 가수부보다 지수부가 훨씬 중요하다는 구글 연구에 따라 `bfloat16` 개발
- `bfloat16` : 구글 브레인에서 개발한 자료형(data type)
	- 구성 : '부호 1비트 - 지수부 8비트 - 가수부 7비트'
	- 딥러닝을 위한 기본 자료형. 따라서 bfloat16을 사용하는 경우 매개변수는 2바이트


#### 양자화, 초거대 모델을 작게 만드는 비밀
- 2바이트 자료형으로 공간을 줄였지만 1바이트로 더 줄일 수 있을까?
	- 줄였을 경우 장점 : GPU를 적게 쓸 수 있어서 비용을 아낄 수 있고 연산속도가 빨라짐. 메모리에서 더 적은 데이터를 읽게 되어 데이터를 이동하는데 드는 시간과 에너지도 줄음 
- 다만, 공간을 줄일수록 정확도는 떨어짐. 따라서 용량을 줄이면서 LLM의 품질을 유지하는 적절한 비율을 찾는게 중요

- 1바이트 사용 시 딥러닝은 실수를 정수로 변환하여 정수 연산으로 처리
	- 변환 방법 : 매개변수의 최댓값을 찾아 127이 되도록 기준값을 구하고 그 기준값을 모든 값에 곱해서 실수 전체를 1바이트 정수형으로 변환
	- 전환하는 과정에서 값의 일부 손실이 발생해서 약간의 사소한 차이 존재
- 매번 변환하기엔 너무 소모적이라서 미리 정수형으로 변환하는데 이처럼 정수로 변환하는 과정을 `양자화(Quantization)`이라고 함. 정수형으로 바꿔둔 모델은 양자화된 모델(Quantization Model)이라고 함.
	- NPU(Neural Processing Unit) 업체들은 엔비디아보다 더 실수 연산을 빠르게 만들기 어려우니 애초에 정수 연산만 가능하도록 코어를 정수 연산기로 채워 양자화된 모델을 빠르게 돌리는 전략을 사용
- 양자화 과정을 거치며 정보의 손실을 최소화하기 위해 유난히 큰 값을 제거해서 다른 값들이 적당하게 분포되도록 처리
	- 양자화를 할 때 가장 큰 값을 기준으로 기준값을 계산하다보니 최대값이 유난히 클 경우 다른 값들의 배치가 너무 비좁아서 모델의 성능이 확연히 떨어지게 됨
	- 따라서 이와 같은 기법을 부분적으로 채택해서 모델의 크기를 줄이면서 성능이 떨어지는 것을 방지


#### 플래시 어텐션, 빛처럼 빠른 속도의 비밀
- GPT-3가 토큰 하나를 생성하기 위해서는 어텐션을 96번씩 동시에 계산하면서 이 과정을 96회 반복(어텐션 총 9216번 계산)
- 이 과정은 엄청나게 무겁고 느린 연산이라서 연산 양을 줄여 보고자 여러 기법 등장

- `플래시 어텐션(Flash Attention)` : 어텐션 계산을 메모리 접근을 최소화하여 GPU 속도와 효율성을 획기적으로 향상시키는 알고리즘 (스탠포드대 박사 과정을 밟던 트리 다오가 제안)
	- 기본동작 원리 : SRAM으로 꼭 필요한 데이터를 조금만 가져와 여기서 읽고 쓰고 하면서 계산
	- 어탠션을 계산하려면 여러 연산 과정을 거치면서 끊임없이 GPU는 메모리를 읽고씀. 따라서 GPS 패키지에는 엄청 빠른 메모리 HBM이 탑재
	- HBM은 기존 CPU 메모리에 비하면 엄청 빠르지만 속도는 SRAM이 훨씬 더 빠름
	- SRAM은 GPU 칩에 내장되어 있어 엄청 빠르지만 용량이 적고 구조가 복잡하고 가격도 비쌈
		- SRAM은 HBM보다 6배 가까이 더 많은 초당 데이터를 전송 할 수 있음
	- HBM은 용량은 크지만 대역폭은 SRAM민큼 크지 않고 SRAM은 대역폭이 큰 대신 용량은 매우 적음

- 플래시 어텐션은 데이터를 한꺼번에 읽어들이고 계산 중에 중간 값은 SRAM에 저장하는 형태로 처리하면서 속도를 높임. 여러 차례 HBM과 주고받은 대신 한 번만 받아 오고 그 다음부터는 SRAM을 이용해 계산한 형태로 계산 방식 변형
	- 여러번 반복해야 하는 계산을 한 번에 처리할 수 있는 형태로 깔끔하게 수식을 정리한 수학연구와 메모리의 특성을 이해하고 HBM 대신 더 빠른 SRAM을 이용하는 기법을 고안한 프로그래밍 기술이 절묘한 조화를 이룬 작품
- 이 결과 최대 10배까지 속도를 높일 수 있었음. 지금은 LLM을 학습하거나 추론할 때 사실상 플래시 어텐션을 필수로 활용해서 훨씬 더 빠른 토큰 생성이 가능


#### KV캐시, 더욱 더 빠르게
- `캐시(Caching)` : 미리 계산한 결과를 저장해 두는 것
- 동일한 계산이 필요할 때 다시 계산 하지 않고 캐시에 저장해둔 값을 꺼내와 불필요한 계산을 줄임. 어텐션을 계산할 때도 캐시 활용
- LLM이 토큰을 생성하는 방식을 떠올리면 문장을 만들 때 문장이 점점 길어져도 이전 토큰에 대한 어탠션 결과는 항상 동일. 왜냐하면 다음 토크는 쳐다보지 않는 마스크드 어텐션으로 진행되기 때문

- `KV캐시(KV Cache)` : 키(Key)와 값(Value)에서 현재 토큰에 대한 값만 계산하고 나머지는 캐시에서 꺼내옴. 쿼리가 이전 값은 가져오지 않고 현재 토큰에 대해서만 계산 진행. 키와 값은 캐시를 활용한다고 하여 KV캐시라고 부름
- 플래시 어텐션과 KV캐시를 적절히 사용하면 토큰 생성 속도를 획기적으로 높일 수 있음


#### 품질 좋은 문장을 생성하는 비밀 옵션
- 어떻게 하면 더 큰 생선 품질을 높여 좋은 문장을 만들어 낼 수 있을까?
- ChatGPT의 경우 전체 토큰의 개수를 대략 20 만개로 추정. LLM은 확률이 가장 높은 토큰을 골라 내는 모델. 그래서 일부 연구자들은 확률적 앵무새라고 비판
	- 언어의 의미는 전혀 이해하지 못한 채 단순히 학습 데이터를 바탕으로 확률적 패턴에 따라 언어를 조합할 뿐
- 따라서 인공지능 모델이 다양한 답변을 할 수 있도록 가장 확률이 높은 토큰만 골라 내는 게 아니라 조금씩 다르게 대답하기 위한 여러 장치를 추가

- `Top K` : 상위 K개만 나열 하고 이중에서만 고르게 함. Top K = 10은 상위 10개 토큰만을 대상으로 한단 뜻
	- 장점 : 엉뚱한 토큰이 나올 가능성이 없음
- `Top P` : 특정 확률 이내. Top P = 80%라면 확률이 높은 순으로 누적하여 전체 합의 80%가 될 때까지 토큰만 고려하겠다는 뜻
	- 확률이 낮은 토큰일수록 품질이 좋지 않은데 확률이 지나치게 낮은 토큰이 나올 가능성을 없앰

- `온도(Temperature)` : LLM이 텍스트를 생성할 때 다음 단어를 선택하는 방식의 무작위성 또는 창의성 수준을 조절하는 매개변수
	- 온도를 높이면 확률이 서로 비슷하게 평준화(더 다양한 답변, 창의성 ↑)
	- 온도를 낮추면 변별력이 높아짐(더 일정한 답변)
	- 온도가 높으면 엔트로피 즉 무질서도 증가. LLM도 온도가 높으면 확률이 서로 비슷해져서 토큰 선택의 무작위성이 높아지니 열역학이나 LLM이나 서로 무질서해 보이는 비슷한 효과가 나옴
- Top K, Top P, 온도 이 3가지 옵션을 잘 기억해두고 ChatGPT를 활용 할 때 사용해보기

 - 이 세가지 외에도 다른 옵션들을 통해 좋은 문장을 만들 수 있음
	- 반복되는 토큰에 패널티를 주기
		 - 동일한 토큰이 반복되면 기계는 좋은 문장이라고 판단할 수 있지만 사람의 입장에서는 결코 좋은 문장이 아니기 때문에 토큰을 반복적으로 사용할 경우 페널티를 부여해 출현 빈도 조절
	 - 생성 할 수 있는 문장이 최대 길이를 제한
		- 한글은 한 글자가 하나의 토큰이므로 1024를 지정 하면 1024 차까지 생성 할 수 있다고 보면 됨


#### 수천 장의 GPU에 분산 학습하는 법
- `추론(Inference)` : 토큰을 생성하는 단계
- `학습(Training)` : 추론 할 수 있는 LLL을 만드는 것
- 앞서 토큰을 생성할 때 속도와 품질을 높이기 위한 다양한 기법처럼 거대한 모델을 학습할 때 속도를 높이기 위한 특별한 기법들 필요

- 효율적으로 학습하기 위해 큰 모델의 경우 다양한 방식을 사용해 하나의 모델을 여러 장의 GPU에 잘라서 보관
    - 모델을 여러 장에 나눠서 보관했을 때 데이터를 GPU 그룹 수만큼 나눈 다음 데이터를 쪼개서 학습. 즉, 데이터를 N분의 1씩 나눠 각 GPU 그룹마다 학습을 진행
		- 수많은 문장 속에 담겨있는 그 다음 토큰과 일치하도록 모델을 학습해 나가는 과정. 일치 한다면 손실이 없는 것이고 일치하지 않는다면 손실 값을 부여 해서 정답을 찾도록 모델을 조정
		- 각자 다른 데이터를 학습한 다음 손실의 평균을 구하고 이 값을 모든 GPS의 동일하게 반영한 다음 다시 다음 데이터 학습을 반복 
- 이전 문장을 기준으로 다음 토큰이 나오도록 데이터를 분산 해서 여러 장의 GP유의 단어 학습 하는 것이 언어 모델 사전학습의 기본 원리
- 이후 사후 학습을 통해 사용자의 프로젝트를 잘 따르도록 만듬 이렇게 만든 모델을 지시 모델이라고 함
    - ChatGPT가 바로 지시 모델의 대표 예
- 정교한 지시모델을 만드는데 RLHF(인간 피드백을 이용한 강화학습) 방식이 큰 역할을 했지만 이 방식은 사람이 일일이 평가하여 구축한 데이터를 필요로 한다는 치명적인 제약이 있음
	- LLM의 가장 큰 혁신은 사람의 개입 없이 엄청나게 많은 텍스트를 학습하기만 하면 된다는 점
- 연구자들 RM이 필요 없고 강화학습도 사용하지 않는 방식은 없을지 고민해왔고 그 중 가장 단순하면서 효과가 좋았던 방식이 스탠포드대 연구팀에서 제안한 DPO(Direct Preference Optimization)
	- RM ->PPO로 이어지는 절차는 모두 생략하고 단순히 기존 SFT 모델을 활용해 DPO 방식 적용
- DPO는 두개의 응답 중 어느 것을 더 선호하는지에 대한 비교 데이터를 사요ㅇ하여 선호하는 응답에 확률을 더 높이는 방식으로 모델 학습
    - DPO 방식으로 학습한 모델은 PPO 방식과 비교의 성능에 큰 차이가 없었으며 학습의 불안정성 문제도 피할 수 있었음


#### 과연 좋은 모델이란?
- 모델의 품질(Performance)을 어떻게 평가 할까?
- 언어 이해 모델은 평가 하기가 쉬웠음. 학습에 사용하지 않은 데이터를 학습이 끝난 모델에 집어넣고 얼마나 분류를 잘하는지 평가하는 방식
- 하지만 생성 모델은 다른 평가 방법이 필요함. 생성된 문장이 좋은 문장인지 어떻게 구별할 수 있을까? 좋다는 그 기준 불분명
- 이전 음성인식 기술에서도 음성을 텍스트로 정확하게 받아 썼는지 평가하는 기준이 필요했고 이를 평가하기 위해 1977년도에 퍼플렉시티(Perplexity)라는 지표 개발
- 이 평가 지표는 기계번역, 텍스트 요약 같은 자연어 처리에도 사용했으며 LLM에서도 언어 생성을 평가하는데 동일하게 사용

- 퍼플렉시티는 정보 이론의 불확실성에 기반
	- 퍼플렉시티가 높으면 혼란스럽고 불확실하다는 뜻
	- 퍼플렉시티가 낮으면 혼란스럽지 않으면 확실하다는 뜻
	- 즉,  좋은 LLM은 퍼플렉시티가 낮음. 이는 생성 되는 토큰의 확률이 매우 높다는 것을 의미

- 다만 퍼플렉시티는 1970년대에 고안된 지표의 만큼 한계도 분명
	- 모델이 복합적으로 올바른 문장을 생성하는지 등은 전혀 평가하지 않고 확률적인 예측 성능만 측정을 하기 때문에 최근에는 간이 지표로만 사용

- 2020년에 UC버클리 연구진이 공개한 사지선다형 문제로 구성된 MMLU(Massive Multitask Language Understanding)라는 평가셋 활용
- 동일한 방식으로 연세대 연구진이 개발한 한국어로 작성된 KMMLU도 있음.  MMLU의 영문 평가셋을 번역한 게 아니라 애초에 한국어로 만든 시험 문제지에서 문제를 추출해서 구성해서 무료로 공개
	- 인문학에서 과학에 이르기까지 45개의 과목에 걸쳐 실제 우리나라 문화에 맞는 3만5천여개 평가 문제로 구성
	- 2024년 하반기 기준 메타의 오픈소스 모델 라마가 65점 기록. 이미 90 점대를 기록 중인 영어에 비해 한글 점수는 아직 낮은 편
- 이외에 개발된 중요한 평가셋:
	- 오픈AI에서 고안한 프로그래밍 코드 생성 능력을 평가하기 위한 `휴먼이밸(HumanEval)`
	- UC버클리 연구진이 개발한 수학 문제 해결 능력을 전문적으로 평가하는 `MATH(Mathematics Aptitude Test of Heuristics)`
	- 오픈AI에서 개발한 8천개가 넘는 초등학교 수학 문제로 구성된 `GSM8K`
- 이처럼 여러가지 평가셋이 공개되어 있는데 가급적 다양한 평가 셋을 동원해서 LLM의 성능을 종합적으로 평가하는 방식이 가장 좋은 평가 방식 


> ***comments***
> - p.134 페이지 하단 그림에서 GPU가 들고 있는 값과 입력값이 뭐야?
> - 텐서 병렬화와 파이프라인 병렬화의 작동 방식에 대해 알려주었는데 말미에 인공지능에 사용했을 때 어떤 차이가 있는지, 어떤 방식을 요즘엔 선호하는지 알려줬으면 좋아겠다라는 생각
> - p.154 메모리의 속도 비교 : CPU 메모리 < HBM < SRAM (Fastest)
>  - p.155 메모리는 컴퓨터의 모든 장치 중에서 가장 빠른 장치 (good fact to know)
> - p. 156 현재 최고 스펙의 SRAM의 성능은 어떨까?
>  - p.157 플래시 어텐션이 수학연구와 메모리 관련 프로그래밍 기술이 절묘한 조화를 이룬 작품이며 어느 한쪽만 알고 있었다면 절대로 탄생 하지 못 했을 기법이라는 말이 울림이 있었음. 인공지능 책을 읽으며 이런 사례를 꽤나 마주했던 것 같은데 혜안은 항상 분야의 융합에 있는 것 같다.
>  - p.163 언어모델이 같은 대답을 안하겠금 방지하는 부분이 나도 궁금했던 부분인데 중요하다면서 조금 얼렁뚱땅 넘어가는 느낌... '가장 높은 확률이 아니라 전체 확률에 따라 대답한다. 아예 모든 토큰에 대해 그냥 확률대로 다음 토큰을 추출한다'는데 그 말은 결국 확률순으로 답한다는 것 아님?
> - p.172 DPO는 선호 데이터를 어떻게 선별해냄? 가끔 챗GPT가 답변 두개 주고 선호도 조사를 하던데 그럼 나는 DPO 비교 데이터셋을 구축하는데 기여하고 있었던 것인가? 이런 식으로 유저한테 선호도를 조사하면 결국 인간이 개입하는 것 아닌가?